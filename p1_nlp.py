# -*- coding: utf-8 -*-
"""p1.NLP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T8OmPhsf7FhPSEX1_EFgPMFz5cCQTK4m

# üìú Project: Job Description Analyzer ‚Äì Extracting Required Skills from Job Postings

## üìå Objective
Use spaCy‚Äôs Named Entity Recognition (NER) and NLTK preprocessing to extract and categorize required skills from job descriptions. The goal is to identify trends in job requirements and analyze the most in-demand skills across industries.

## üõ†Ô∏è Project Steps & Instructions
"""

#üì• Download the Dataset
!wget https://raw.githubusercontent.com/binoydutt/Resume-Job-Description-Matching/refs/heads/master/data.csv

"""### Step 1: Load the Dataset
#### üìå Dataset: A provided CSV file containing job descriptions from different industries (IT, Healthcare, Finance, Marketing, etc.).

1. Download the dataset (link below).
2. Load it into Python using Pandas.
3. View the first few rows to understand its structure.
"""

# your code here
import pandas as pd
df = pd.read_csv('data.csv.1')
df.head()

"""### Step 2: Preprocessing the Job Descriptions
#### üìå Goal: Clean the text by removing stopwords, punctuation, and unnecessary characters.

1. Use NLTK to tokenize the descriptions.
2. Remove stopwords and special characters.
3. Convert text to lowercase for consistency.
"""

# your code here

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize


nltk.download('punkt')
nltk.download('stopwords')


# Define the preprocessing function
def preprocess_text(text):
    text = text.lower()
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]

    # Join the tokens back into a single string
    preprocessed_text = ' '.join(tokens)

    return preprocessed_text


df = pd.read_csv('data.csv')

# Apply preprocessing to the job descriptions
df['cleaned_description'] = df['Job Description'].apply(preprocess_text)

df[['Job Description', 'cleaned_description']].head()

"""### Step 3: Extract Skills Using Named Entity Recognition (NER)
#### üìå Goal: Use spaCy‚Äôs built-in NER to detect and extract skills from job descriptions.

1. Load spaCy‚Äôs English model.
2. Use NER to identify important keywords.
3. Extract words related to technical skills, tools, and expertise.
"""

!pip install spacy
!python -m spacy download en_core_web_sm

import spacy

nlp = spacy.load('en_core_web_sm')

def extract_skills(text):
    doc = nlp(text)
    relevant_labels = ['technical skills', 'tools', 'expertise']

    # Extract entities with relevant labels
    skills = [ent.text for ent in doc.ents if ent.label_ in relevant_labels]

    return skills


df['extracted_skills'] = df['cleaned_description'].apply(extract_skills)

df[['cleaned_description', 'extracted_skills']].head()

"""### Step 4: Identify the Most In-Demand Skills
#### üìå Goal: Count the most frequently mentioned skills in job descriptions.

1. Create a word frequency distribution of extracted skills.
2. Identify the top 10 most required skills.
"""

all_skills = [skill for sublist in df['extracted_skills'] for skill in sublist]

from collections import Counter

# Count the frequency of each skill
skill_counts = Counter(all_skills)

# Get the top 10 most common skills
top_skills = skill_counts.most_common(10)


print(top_skills)

"""### Step 5: Categorize Skills by Industry
#### üìå Goal: Compare the most in-demand skills across different industries.

1. Group job descriptions by industry.
2. Extract and analyze skills for each industry.
3. Compare IT vs. Marketing vs. Healthcare, etc..
"""

# Group the dataset by industry
industry_groups = df.groupby('industry')

# Create a dictionary to store skills for each industry
industry_skills = {}

# Extract skills for each industry
for industry, group in industry_groups:
    # Flatten the list of extracted skills for the industry
    all_skills = [skill for sublist in group['extracted_skills'] for skill in sublist]
    # Count the frequency of each skill
    skill_counts = Counter(all_skills)
    # Store the top 10 skills for the industry
    industry_skills[industry] = skill_counts.most_common(10)

# Display the top skills for each industry
for industry, skills in industry_skills.items():
    print(f"Top 10 Skills in {industry}:")
    for skill, count in skills:
        print(f"{skill}: {count}")
    print()




print(3357755332)


import pandas as pd
from collections import Counter


df = df.dropna(subset=['extracted_skills'])


df['extracted_skills'] = df['extracted_skills'].apply(lambda x: eval(x) if isinstance(x, str) else x)



df_exploded = df.explode('extracted_skills')



industry_skills = (
    df_exploded.groupby('industry')['extracted_skills']
    .apply(lambda x: Counter(x).most_common(10))
    .to_dict()
)


for industry, skills in industry_skills.items():
    print(f"Top 10 Skills in {industry}:")
    for skill, count in skills:
        print(f"{skill}: {count}")
    print()